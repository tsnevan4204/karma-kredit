{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9886f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47bf2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tx_matrix = np.load(\"X_tx_matrix.npy\")\n",
    "y = np.load(\"y_fico_scores.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd51e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tx_matrix = np.nan_to_num(X_tx_matrix, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "y = np.nan_to_num(y, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07e19e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature variances: 2.1745524e+18\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature variances:\", X_tx_matrix.var(axis=(0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2d8f678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Using device: cpu\n",
      "üì• Loading data...\n",
      "‚úÖ X shape: (100, 4), y shape: (1,)\n",
      "üßº Cleaning data...\n",
      "NaNs remaining in X: 0\n",
      "Feature variances before scaling: 2.174552387325264e+18\n",
      "Feature variances after scaling: 1.0\n",
      "X_tensor shape: torch.Size([100, 4])\n",
      "y_tensor shape: torch.Size([1])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_tensor shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_tensor shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 44\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Simple MLP model\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/onchain-fico/.venv/lib/python3.9/site-packages/torch/utils/data/dataset.py:204\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "# Load data\n",
    "print(\"üì• Loading data...\")\n",
    "X_tx_matrix = np.load(\"X_tx_matrix.npy\")\n",
    "y = np.load(\"y_fico_scores.npy\")\n",
    "print(f\"‚úÖ X shape: {X_tx_matrix.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Clean and scale\n",
    "print(\"üßº Cleaning data...\")\n",
    "X_tx_matrix = np.nan_to_num(X_tx_matrix, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "y = np.nan_to_num(y, nan=0.0)\n",
    "\n",
    "print(f\"NaNs remaining in X: {np.isnan(X_tx_matrix).sum()}\")\n",
    "print(f\"Feature variances before scaling: {X_tx_matrix.var(axis=(0, 1))}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "flat = X_tx_matrix.reshape(-1, X_tx_matrix.shape[-1])\n",
    "flat_scaled = scaler.fit_transform(flat)\n",
    "X_tx_matrix_scaled = flat_scaled.reshape(X_tx_matrix.shape)\n",
    "\n",
    "print(f\"Feature variances after scaling: {X_tx_matrix_scaled.var(axis=(0, 1))}\")\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_tensor = torch.tensor(X_tx_matrix_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "print(\"X_tensor shape:\", X_tensor.shape)\n",
    "print(\"y_tensor shape:\", y_tensor.shape)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Simple MLP model\n",
    "class MLPFICO(nn.Module):\n",
    "    def __init__(self, input_dim=4, seq_len=100):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim * seq_len, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = MLPFICO().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training\n",
    "print(\"\\nüèãÔ∏è Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for batch_i, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        preds = model(batch_x)\n",
    "\n",
    "        if torch.isnan(preds).any():\n",
    "            print(\"‚ùå NaN detected in predictions ‚Äî check inputs!\")\n",
    "            break\n",
    "\n",
    "        loss = criterion(preds, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        if batch_i % 10 == 0:\n",
    "            print(f\"üåÄ Epoch {epoch+1}, Batch {batch_i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1} finished. Avg Loss: {np.mean(epoch_losses):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nüß™ Evaluating model...\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        preds = model(batch_x)\n",
    "\n",
    "        if torch.isnan(preds).any():\n",
    "            print(\"‚ùå NaNs detected in evaluation predictions\")\n",
    "            break\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(batch_y.cpu().numpy())\n",
    "\n",
    "# Flatten arrays\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "\n",
    "print(f\"üìè Predictions: mean={all_preds.mean():.2f}, std={all_preds.std():.2f}\")\n",
    "print(f\"üéØ Targets:     mean={all_targets.mean():.2f}, std={all_targets.std():.2f}\")\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(all_targets, all_preds)\n",
    "r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "print(\"\\nüìä Evaluation Metrics:\")\n",
    "print(f\"MAE:  {mae:.2f}\")\n",
    "print(f\"R¬≤:   {r2:.3f}\")\n",
    "\n",
    "# Prediction samples\n",
    "print(\"\\nüîç Sample Predictions:\")\n",
    "for i in range(10):\n",
    "    print(f\"True: {all_targets[i]:.1f}, Predicted: {all_preds[i]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4bcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Simple Model Metrics:\n",
      "MAE: 15.29\n",
      "R¬≤:  0.924\n",
      "\n",
      "üîç Sample Predictions:\n",
      "True: 350.0, Pred: 316.9\n",
      "True: 500.0, Pred: 491.0\n",
      "True: 543.0, Pred: 538.5\n",
      "True: 489.0, Pred: 533.3\n",
      "True: 515.0, Pred: 516.0\n",
      "True: 545.0, Pred: 548.6\n",
      "True: 589.0, Pred: 626.6\n",
      "True: 730.0, Pred: 661.7\n",
      "True: 642.0, Pred: 627.6\n",
      "True: 419.0, Pred: 421.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge  # LinearRegression if you don't want L2\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Load data\n",
    "X_tx = np.load(\"sim_data/X_tx_matrix.npy\")              # (N, 100, 4)\n",
    "X_wallet = np.load(\"sim_data/X_wallet_features.npy\")    # (N, 5)\n",
    "y = np.load(\"sim_data/y_fico_scores.npy\")               # (N,)\n",
    "\n",
    "# Clean NaNs/Infs\n",
    "X_tx = np.nan_to_num(X_tx, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "X_wallet = np.nan_to_num(X_wallet, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "y = np.nan_to_num(y, nan=0.0)\n",
    "\n",
    "# Feature engineering\n",
    "# Pool transaction features into simple summary stats\n",
    "X_tx_mean = X_tx.mean(axis=1)     # (N, 4)\n",
    "X_tx_std = X_tx.std(axis=1)       # (N, 4)\n",
    "\n",
    "X_combined = np.concatenate([X_tx_mean, X_tx_std, X_wallet], axis=1)  # (N, 13)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "# Train/test split (optional ‚Äî full train for now)\n",
    "model = Ridge(alpha=1.0)  # Use LinearRegression() if you want no regularization\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_scaled)\n",
    "\n",
    "# Eval\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"\\nüìä Simple Model Metrics:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R¬≤:  {r2:.3f}\")\n",
    "\n",
    "print(\"\\nüîç Sample Predictions:\")\n",
    "for i in range(10):\n",
    "    print(f\"True: {y[i]:.1f}, Pred: {y_pred[i]:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7030c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä XGBoost Model Evaluation:\n",
      "MAE: 7.34\n",
      "R¬≤:  0.985\n",
      "\n",
      "üîç Sample Predictions:\n",
      "True: 350.0, Pred: 349.8\n",
      "True: 500.0, Pred: 512.4\n",
      "True: 543.0, Pred: 553.5\n",
      "True: 489.0, Pred: 483.9\n",
      "True: 515.0, Pred: 525.7\n",
      "True: 545.0, Pred: 555.9\n",
      "True: 589.0, Pred: 586.6\n",
      "True: 730.0, Pred: 723.4\n",
      "True: 642.0, Pred: 647.3\n",
      "True: 419.0, Pred: 433.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load data\n",
    "X_tx = np.load(\"sim_data/X_tx_matrix.npy\")              # shape: (N, 100, 4)\n",
    "X_wallet = np.load(\"sim_data/X_wallet_features.npy\")    # shape: (N, 5)\n",
    "y = np.load(\"sim_data/y_fico_scores.npy\")               # shape: (N,)\n",
    "\n",
    "# Clean invalid values\n",
    "X_tx = np.nan_to_num(X_tx, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "X_wallet = np.nan_to_num(X_wallet, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "y = np.nan_to_num(y, nan=0.0)\n",
    "\n",
    "# Feature engineering: mean & std over transaction sequences\n",
    "X_tx_mean = X_tx.mean(axis=1)  # shape: (N, 4)\n",
    "X_tx_std = X_tx.std(axis=1)    # shape: (N, 4)\n",
    "\n",
    "# Combine all features\n",
    "X_combined = np.concatenate([X_tx_mean, X_tx_std, X_wallet], axis=1)  # shape: (N, 13)\n",
    "\n",
    "# Standardize input features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_combined)\n",
    "\n",
    "# Define and train XGBoost model\n",
    "final_model = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    objective=\"reg:squarederror\",\n",
    "    verbosity=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "final_model.fit(X_scaled, y)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = final_model.predict(X_scaled)\n",
    "\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"\\nüìä XGBoost Model Evaluation:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R¬≤:  {r2:.3f}\")\n",
    "\n",
    "print(\"\\nüîç Sample Predictions:\")\n",
    "for i in range(10):\n",
    "    print(f\"True: {y[i]:.1f}, Pred: {y_pred[i]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5b30920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: fico_xgb_model.pkl and fico_xgb_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "with open(\"fico_xgb_model.pkl\", \"wb\") as f_model:\n",
    "    pickle.dump(final_model, f_model)\n",
    "\n",
    "# Save scaler used on X_combined\n",
    "with open(\"fico_xgb_scaler.pkl\", \"wb\") as f_scaler:\n",
    "    pickle.dump(scaler, f_scaler)\n",
    "\n",
    "print(\"üíæ Saved: fico_xgb_model.pkl and fico_xgb_scaler.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onchain-fico",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
